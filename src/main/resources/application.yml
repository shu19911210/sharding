server:
  port: 8900
  servlet:
    context-path: /moku-planet-web
    session:
      timeout: PT1H
#spring:
#  shardingsphere:
#    datasource:
#      names: ds0,ds1
#      ds0:
#        username: root
#        password: root123
#        url: jdbc:mysql://192.168.31.46:3306/moku_planet?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=GMT%2B8&allowPublicKeyRetrieval=true&allowMultiQueries=true
#        driver-class-name: com.mysql.cj.jdbc.Driver
#        type: com.alibaba.druid.pool.DruidDataSource
#        # 下面为连接池的补充设置，应用到上面所有数据源中
#        # 初始化大小，最小，最大
#        initial-size: 5
#        min-idle: 5
#        max-active: 20
#        # 配置获取连接等待超时的时间
#        max-wait: 60000
#        # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
#        time-between-eviction-runs-millis: 60000
#        # 配置一个连接在池中最小生存的时间，单位是毫秒
#        min-evictable-idle-time-millis: 300000
#        validation-query: SELECT 1 FROM DUAL
#        test-while-idle: true
#        test-on-borrow: false
#        test-on-return: false
#        # 打开PSCache，并且指定每个连接上PSCache的大小
#        pool-prepared-statements: true
#        #   配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
#        max-pool-prepared-statement-per-connection-size: 20
#        filters: stat,wall
#        use-global-data-source-stat: true
#        # 通过connectProperties属性来打开mergeSql功能；慢SQL记录
#        connect-properties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
#      ds1:
#        username: root
#        password: root123
#        url: jdbc:mysql://127.0.0.1:3306/briefly?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=GMT%2B8&allowPublicKeyRetrieval=true&allowMultiQueries=true
#        driver-class-name: com.mysql.cj.jdbc.Driver
#        type: com.alibaba.druid.pool.DruidDataSource
#        # 下面为连接池的补充设置，应用到上面所有数据源中
#        # 初始化大小，最小，最大
#        initial-size: 5
#        min-idle: 5
#        max-active: 20
#        # 配置获取连接等待超时的时间
#        max-wait: 60000
#        # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
#        time-between-eviction-runs-millis: 60000
#        # 配置一个连接在池中最小生存的时间，单位是毫秒
#        min-evictable-idle-time-millis: 300000
#        validation-query: SELECT 1 FROM DUAL
#        test-while-idle: true
#        test-on-borrow: false
#        test-on-return: false
#        # 打开PSCache，并且指定每个连接上PSCache的大小
#        pool-prepared-statements: true
#        #   配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
#        max-pool-prepared-statement-per-connection-size: 20
#        filters: stat,wall
#        use-global-data-source-stat: true
#        # 通过connectProperties属性来打开mergeSql功能；慢SQL记录
#        connect-properties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
#    sharding:
#      tables:
#        task_data_submit_record_briefly:
#          actual-data-nodes: ds$->{0..1}.task_data_submit_record_briefly
#          key-generator:
#            column: task_data_submit_record_id
#            type: SNOWFLAKE
spring:
  main:
    allow-bean-definition-overriding: true
  shardingsphere:
    #参数配置 显示sql
    props:
      sql:
        show: true
    #配置数据源
    datasource:
      #给每个数据源取别名  下面的ds1 ds2 ds3任意取别名
      names: ds0,ds1
      #给master-ds1 每个数据源配置数据库连接信息
      ds0:
        #配置druid数据源
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://192.168.31.46:3306/moku_planet?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=GMT%2B8&allowPublicKeyRetrieval=true&allowMultiQueries=true
        username: root
        password: root123
        maxPoolSize: 100
        minPoolSize: 5
      #配置ds2-slave丛库
      ds1:
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://127.0.0.1:3306/briefly?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=GMT%2B8&allowPublicKeyRetrieval=true&allowMultiQueries=true
        username: root
        password: root123
        maxPoolSize: 100
        minPoolSize: 5
        #默认数据源 主要用于写 注意一定要配置读写分离 注意 如果不配置 那么就会把上面两个节点当做从slav节点e
    sharding:
      default-data-source-name: ds0
      tables:
        #逻辑表名
        task_data_submit_record_briefly:
          #主键的列明
          column: task_data_submit_record_id
          type: SNOWFLAKE
          # 数据节点 数据源$->{0..N}.逻辑表名$->{0..N}
          actual-data-nodes: ds$->{0..1}.task_data_submit_record_briefly$->{0..1}
          # 拆分库策略 也就是什么样的数据放入到哪个数据库中
          database-strategy:
            inline:
              # 分片字段（分片键）
              sharding-column: task_data_submit_record_id
              # 分片算法表达式
              algorithm-expression: ds$->{task_data_submit_record_id % 2}
          table-strategy:
            inline:
              sharding-column: task_data_submit_record_id
              algorithm-expression: task_data_submit_record_briefly$->{task_data_submit_record_id % 2}
        binding-tables[0]: task_data_submit_record_briefly,task_data

#mybatis-plus配置
mybatis-plus:
  typeAliasesPackage: com.study.sharding.entity #实体类位置
  mapper-locations: classpath:/mapper/*.xml #xml扫描，多个目录用逗号或者分号分隔（告诉 Mapper 所对应的 XML 文件位置）
  configuration:
    map-underscore-to-camel-case: true #是否开启自动驼峰命名规则映射:从数据库列名到Java属性驼峰命名的类似映射
    call-setters-on-nulls: true #如果查询结果中包含空值的列，则 MyBatis 在映射的时候，不会映射这个字段
    #    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl #这个配置会将执行的sql打印出来，在开发或测试的时候可以用
